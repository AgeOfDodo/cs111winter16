

Q 1.1 why does it take this many threads or iterations?
1. Once I started running the program with ~50 threads and ~150 iterations, the count would be incorrect 
	roughly half the time. Once I changed it to about ~60 threads and roughly ~500 iterations, the count
	would rarely return correct.
	It takes this many threads and iterations because with a smaller number of threads, there are very
	few conflicts of calling add() at the same time. However, as the number of iterations increases, the chance 
	that more than one thread collides with another thread's attempt to add() increases greatly. 
2. A significantly smaller number of iterations so seldom fails because there is a greatly decreased
	chance of threads colliding in their call to add(). 


Q 1.2
1. A lot of the time picked up by the clock_gettime calls is the overhead needed to create the threads.
	as more threads are added, the overhead value stays roughly the same, and so the average drops
	considerably.
2. In order to get the "correct" time, we could add a second pair of clock_gettime function calls and 
	subtract this value from the current value for time we're receiving, as this would get rid of the overhead
	that calling pthread_create causes. 
3. --yield runs are so much slower because for every iteration the program must yield, thus giving up 
	the CPU and allowing context switches to occur. This added time as the iterations increase is
	likely do to a greatly growing number of context switches.
	

4. No, we can't get valid timing when we use --yield because (1) there is a lot of overhead when we do context switching. 
	(2) it is different from the previous problem with pthread_create, where we simply gettime around pthread_create. With 
	--yield, since yield() give up CPU time for another thread and does not return to the main function right away, we can't do
	get time ().

Q 1.3
1. All options perform similarly for low numbers because there aren't many collisions and therefore
	isnt that much waiting time for any blocked thread, and thus all methods perform similarly.
2. As the number of threads increase, the more threads exist that need to be blocked. This means
	that threads begin to spend time waiting and thus the average time per operation rises.
3. Spin-locks do busy waiting, and therefore cost a lot of time. 



Q 2.1
time per operation drops as the number of iterations increases. This is becasue the overhead of creating threads takes up a smaller percentage as it is distributed to each operation in our calulation.
We can fix this problem by measuring the time it takes to create a thread (gettime before pthread_create and after pthread_create) and subtracting it from our calculation. 

To demostrate conflicts between inserts(--yield=i), we have 16 threads and 150 iterations. To check that the conflicts actually happen between inserts, we print out a "before" line before each call to insert() and an "after" line after. If there are several "before" lines printed out right before seg fault, then we know that the conflict happens between inserts.

Using the same mothod, we can demostrate conflicts between deletes(yield=d), inserts and lookups (yield=is), and delete and lookup(yield=ds). 	

When using --sync=m or --sync=s, we don't have seg fault.
See graph.pdf for graphs. 

Q 2.2
    (a) Compare the variation in time per protected operation vs the number of threads in Part 2 and in Part 1. Explain the difference.*/
		The spinlocks have different results for two graphs. Spinlocks in part 1 increase as the number of threads increases, and spinlocks in part 2 decrease as the number of threads increases.

--lists implementation:
	We dynamically allocate an array of (sub)lists and two arrays of locks(m and s). 
	The new length function take argument of the pointer to the lists array and use a loop to add up the length of each sub-list.
	Before the thread function call length(), it must acquire all the locks. To make sure that a thread don't reach a deadlock when trying to acquire all locks, we declare additional lock such that only the thread that acquire this lock can proceed to acquire other sub-locks. See threadFunc() for implementation.


Q 2.3

	(a) Explain the the change in performance of the synchronized methods as a function of the number of threads per list.

	As the ratio increases, the time per operations increases. This can be explain with pigeonhole theorem. When #lists < #threads, some threads will have to wait for the others to acquire locks for a same sub-lists.(it's like the pigeonhole theory!). But when #lists > #threads, each list can be assigned to a specific thread such that all the operations on a sublist is all done by one assigned thread. There will be some threads that need to handle two or more lists, but no threads have to wait for another threads. 
	
	(b) Explain why threads per list is a more interesting number than threads (for this particular measurement).
	If we use the number of threads to draw the graph, then we wouldn't find this cool behavior, therefore the ratio is a more interesting number.

Q 3.1
	(a). Why must the mutex be held when pthread_cond_wait is called?
		One must obtain the lock before calling pthread_cond_wait because pthread_cond_wait() unlock the lock. It is an undefine behavior if we try to unlock an unlocked lock.
	
	(b). Why must the mutex be released when the waiting thread is blocked?

		The mutex must be released when the thread is waiting because other threads may need to acquire the mutex to change the desire condition. If thread a holds a mutex and waits for a condition to change while thread b try to acquire the mutex to signal the condition, then deadlock.

	(c). Why must the mutex be reacquired when the calling thread resumes?

		The mutex must be reacquired becuase the whole point of having pthread_cond_wait is that we want the thread to hold the mutex all the time except when a condition is not met and need other threads to change the predicate. 

	(d). Why must this be done inside of pthread_cond_wait? Why canâ€™t the caller simply release the mutex before calling pthread_cond_wait?

		Race condition. If the operation were not atomic, then race condition will occur. For example: Say thread a need the  condition a == 1, and thread b can change a to 0 or 1.
		time	thread a 		thread b 
		|		unlock 			
		|						a = 1
		V	 					a = 0
				read condition

		Thead a might miss the condition, and we don't want that.

	(e). Can this be done in a user-mode implementation of pthread_cond_wait? If so, how? If it can only be implemented by a system call, explain why?

		I can only be implemented by a system call because the condition may involove permission to access to kernel.
		For example, a predicate may involove success on reading a file.
